{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5f5f33d",
   "metadata": {},
   "source": [
    "# Patient Similarity Graph Using GNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6f2e592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "use_cuda_if_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3679616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_cols(df, p=80):\n",
    "    #1- count the number of NaN values in each column\n",
    "    #2- calculate the percentage of NaN values in each column\n",
    "    #3- get the list of columns to drop\n",
    "    #4- drop the columns with more than 80% NaN values\n",
    "    nan_counts = df.isna().sum()    \n",
    "    nan_percentages = nan_counts / len(df) * 100 \n",
    "    cols_to_drop = nan_percentages[nan_percentages > p].index.tolist()\n",
    "    df = df.drop(cols_to_drop, axis=1)\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37c117",
   "metadata": {},
   "source": [
    "# Reading Lung dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9411bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'E:\\VCU 2023\\PSN Patient Similarity Network\\GraphAugmentation'\n",
    "original_lung = pd.read_csv(f'{path}/data/Lung/numerical.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08e138e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F1', 'F2', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F12', 'F13', 'F16', 'F18', 'F19', 'F23', 'F24']\n"
     ]
    }
   ],
   "source": [
    "original_features = list(original_lung.columns)\n",
    "\n",
    "new_features      = [f'F{i}' for i in range(len(original_features))]\n",
    "features_dict     = {new_features[i]: list(original_features)[i] for i in range(len(original_features))}\n",
    "\n",
    "Lung = original_lung\n",
    "Lung = Lung.rename(columns=dict(zip(original_features, new_features)))\n",
    "Lung = dropping_cols(Lung)\n",
    "\n",
    "# Imputing the NaN values to the mean\n",
    "features_to_impute = [i for i in list(Lung.columns) if i not in ['F11', 'F20','F21','F22']]\n",
    "print(features_to_impute)\n",
    "Lung[features_to_impute] = Lung[features_to_impute].fillna(Lung[features_to_impute].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ace38ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([773, 15])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ahmad Al Musawi\\AppData\\Local\\Temp\\ipykernel_19376\\477567934.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  temp.append(torch.tensor(col))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 715,  800, 1515,    0,  776,  800, 6887,    0, 2431, 4263,    0, 5506,\n",
       "           0, 2642, 4778], dtype=torch.int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = Lung[features_to_impute]\n",
    "df = df.mul(100).round().astype(int)\n",
    "X = torch.tensor(df.values)\n",
    "num_classes  = X.shape[0]\n",
    "num_features = X.shape[1]\n",
    "print(X.shape)\n",
    "tensors = []\n",
    "for row in X:\n",
    "    temp = []\n",
    "    for col in row:\n",
    "        temp.append(torch.tensor(col))\n",
    "    tensors.append(torch.tensor(temp))\n",
    "\n",
    "X = torch.stack(tensors)\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e0feb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100,   0,   0, ...,   0,   0,   0],\n",
       "       [  0, 100,   0, ...,   0,   0,   0],\n",
       "       [  0,   0, 100, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   0,   0, ..., 100,   0,   0],\n",
       "       [  0,   0,   0, ...,   0, 100,   0],\n",
       "       [  0,   0,   0, ...,   0,   0, 100]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the similarity matrix SM of the given datafram \n",
    "SM = pd.DataFrame(1/(1 + squareform(pdist(df, 'euclidean'))), index=df.index, columns=df.index).values\n",
    "SM = (SM * 100).round().astype(int)\n",
    "SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89006c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2467])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = []\n",
    "target = []\n",
    "weight = []\n",
    "\n",
    "similarity_threshold = 0\n",
    "\n",
    "for i in range(SM.shape[0]):\n",
    "    for j in range(SM.shape[1]):\n",
    "        if SM[i,j]> similarity_threshold:\n",
    "            source.append(i)\n",
    "            target.append(j)\n",
    "            weight.append(SM[i,j])\n",
    "\n",
    "edge_index = torch.tensor([source, target])\n",
    "edge_index.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c6e8ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([773])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = {'NSCLC'        : 0,\n",
    "     'NSCLC Surgery': 1,\n",
    "     'SCLC'         : 2}\n",
    "Y = torch.tensor([v[i] for i in list(Lung['F22'])])\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb7d14a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mask(n, tr=0.8, vl=0.1, ts=0.1):\n",
    "    import random\n",
    "    train_size = int(n * tr)\n",
    "    val_size = int(n * vl)\n",
    "    test_size = int(n * ts)\n",
    "\n",
    "    # Initialize the three lists\n",
    "    train_list = torch.zeros(n, dtype=torch.bool)\n",
    "    val_list   = torch.zeros(n, dtype=torch.bool)\n",
    "    test_list  = torch.zeros(n, dtype=torch.bool)\n",
    "\n",
    "    indices = [i for i in range(n)]\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    for i in range(n):\n",
    "        j = indices[i]\n",
    "        if i <train_size:\n",
    "            train_list[j] = torch.tensor(True)\n",
    "        elif i>= train_size and i< train_size + val_size:\n",
    "            val_list[j] = torch.tensor(True)\n",
    "        elif i>=train_size + val_size:\n",
    "            test_list[j] = torch.tensor(True)\n",
    "    return train_list, val_list, test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5437232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "tr_mask, v_mask, ts_mask = split_mask(X.shape[0])\n",
    "# print(tr_mask)\n",
    "# print(v_mask)\n",
    "# print(ts_mask)\n",
    "\n",
    "data = Data(x=X, edge_index = edge_index, y = Y, train_mask = tr_mask, val_mask = v_mask, test_mask = ts_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "054f0878",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 715,  800, 1515,  ...,    0, 2642, 4778],\n",
       "        [ 668,  900, 1449,  ...,  600, 2813, 4712],\n",
       "        [ 494, 1000, 1657,  ...,  600, 3014, 4780],\n",
       "        ...,\n",
       "        [ 683,  600, 1661,  ...,    0, 2710, 5957],\n",
       "        [ 359,  600, 1238,  ...,    0, 2272, 4392],\n",
       "        [ 280, 1000,  819,  ...,    0, 1543, 2712]], dtype=torch.int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # print(type(dataset.data.train_mask))\n",
    "# # print(type(dataset.data.val_mask))\n",
    "# # print(type(dataset.data.test_mask))\n",
    "# print(data.x)\n",
    "# print(\"edge_index:\\t\\t\",data.edge_index.shape)\n",
    "# print(data.edge_index)\n",
    "# print(\"\\n\")\n",
    "# print(\"train_mask:\\t\\t\",data.train_mask.shape)\n",
    "# print(data.train_mask)\n",
    "# print(\"\\n\")\n",
    "# print(\"x:\\t\\t\",data.x.shape)\n",
    "# print(data.x)\n",
    "# print(\"\\n\")\n",
    "# print(\"y:\\t\\t\",data.y.shape)\n",
    "# print(data.y)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8347d990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07219035",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "#         self.conv = SAGEConv(dataset.num_features,\n",
    "#                              dataset.num_classes,\n",
    "#                              aggr=\"max\") # max, mean, add ...)\n",
    "        self.conv = SAGEConv(num_features,\n",
    "                             num_classes,\n",
    "                             aggr=\"max\") # max, mean, add ...)\n",
    "\n",
    "    def forward(self):\n",
    "        x = self.conv(data.x, data.edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7812d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() and use_cuda_if_available else 'cpu')\n",
    "model, data = Net().to(device), data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b544c057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c393a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "#     print(data.x[0])\n",
    "#     print(data.edge_index)\n",
    "#     _ = input()\n",
    "\n",
    "    F.nll_loss(model()[data.train_mask], data.y[data.train_mask]).backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    logits, accs = model(), []\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        accs.append(acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0de7c8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 must have the same dtype",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m test_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     _, val_acc, tmp_test_acc \u001b[38;5;241m=\u001b[39m test()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val_acc \u001b[38;5;241m>\u001b[39m best_val_acc:\n",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#     print(data.x[0])\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     print(data.edge_index)\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#     _ = input()\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     F\u001b[38;5;241m.\u001b[39mnll_loss(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[data\u001b[38;5;241m.\u001b[39mtrain_mask], data\u001b[38;5;241m.\u001b[39my[data\u001b[38;5;241m.\u001b[39mtrain_mask])\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Documents\\anaconda3\\envs\\venvGNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlog_softmax(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Documents\\anaconda3\\envs\\venvGNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Documents\\anaconda3\\envs\\venvGNN\\lib\\site-packages\\torch_geometric\\nn\\conv\\sage_conv.py:132\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[1;34m(self, x, edge_index, size)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m--> 132\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_l\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight \u001b[38;5;129;01mand\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Documents\\anaconda3\\envs\\venvGNN\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Documents\\anaconda3\\envs\\venvGNN\\lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:132\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 must have the same dtype"
     ]
    }
   ],
   "source": [
    "best_val_acc = test_acc = 0\n",
    "for epoch in range(1,100):\n",
    "    train()\n",
    "    _, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    log = 'Epoch: {:03d}, Val: {:.4f}, Test: {:.4f}'\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(log.format(epoch, best_val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb902f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94ff6a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b2883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9365484",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c28fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3707a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
